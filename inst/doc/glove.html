<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>GloVe Word Embeddings</title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
section.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
</head>
<body>
<div class="include-before">
</div>
<div class="frontmatter">
<div class="title"><h1>GloVe Word Embeddings</h1></div>
<div class="author"><h2>Dmitriy Selivanov</h2></div>
<div class="date"><h3>2023-11-09</h3></div>
</div>
<div class="body">
<h1 id="word-embeddings">Word embeddings</h1>
<p>After Tomas Mikolov et al. released the <a href="https://code.google.com/archive/p/word2vec">word2vec</a> tool, there was a boom of articles about word vector representations. One of the best of these articles is Stanford’s <a href="http://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a>, which explained why such algorithms work and reformulating word2vec optimizations as a special kind of factorization for word co-occurence matrices.</p>
<p>Here I will briefly introduce the GloVe algorithm and show how to use its text2vec implementation.</p>
<h1 id="glove-algorithm">GloVe algorithm</h1>
<p>THe GloVe algorithm consists of following steps:</p>
<ol>
<li>
<p>Collect word co-occurence statistics in a form of word co-ocurrence matrix \(X\). Each element \(X_{ij}\) of such matrix represents how often word <em>i</em> appears in context of word <em>j</em>. Usually we scan our corpus in the following manner: for each term we look for context terms within some area defined by a <em>window_size</em> before the term and a <em>window_size</em> after the term. Also we give less weight for more distant words, usually using this formula: $$decay = 1/offset$$</p>
</li>
<li>
<p>Define soft constraints for each word pair:  $$w_i^Tw_j + b_i + b_j = log(X_{ij})$$ Here \(w_i\) - vector for the main word, \(w_j\) - vector for the context word, \(b_i\), \(b_j\) are scalar biases for the main and context words.</p>
</li>
<li>
<p>Define a cost function
$$J = \sum_{i=1}^V \sum_{j=1}^V \; f(X_{ij}) ( w_i^T w_j + b_i + b_j - \log X_{ij})^2$$
Here \(f\) is a weighting function which help us to prevent learning only from extremely common word pairs. The GloVe authors choose the following function:</p>
</li>
</ol>
<p>$$
f(X_{ij}) =
\begin{cases} (\frac{X_{ij}}{x_{max}})^\alpha &amp; \text{if } X_{ij} &lt; XMAX \\ 1 &amp; \text{otherwise} \end{cases}
$$</p>
<h1 id="linguistic-regularities">Linguistic regularities</h1>
<p>Now let’s examine how GloVe embeddings works. As commonly known, word2vec word vectors capture many linguistic regularities. To give the canonical example, if we take word vectors for the words “paris,” “france,” and “germany” and perform the following operation:</p>
<p>$$vector(&quot;paris&quot;) - vector(&quot;france&quot;) + vector(&quot;germany&quot;)$$</p>
<p>the resulting vector will be close to the vector for “berlin”.</p>
<p>Let’s download the same Wikipedia data used as a demo by word2vec:</p>
<pre><code class="language-r">library(text2vec)
text8_file = &quot;~/text8&quot;
if (!file.exists(text8_file)) {
  download.file(&quot;http://mattmahoney.net/dc/text8.zip&quot;, &quot;~/text8.zip&quot;)
  unzip (&quot;~/text8.zip&quot;, files = &quot;text8&quot;, exdir = &quot;~/&quot;)
}
wiki = readLines(text8_file, n = 1, warn = FALSE)
</code></pre>
<p>In the next step we will create a vocabulary, a set of words for which we want to learn word vectors. Note, that all of text2vec’s functions which operate on raw text data (<code>create_vocabulary</code>, <code>create_corpus</code>, <code>create_dtm</code>, <code>create_tcm</code>) have a streaming API and you should iterate over tokens as the first argument for these functions.</p>
<pre><code class="language-r"># Create iterator over tokens
tokens &lt;- space_tokenizer(wiki)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab &lt;- create_vocabulary(it)
</code></pre>
<p>These words should not be too uncommon. Fot example we cannot calculate a meaningful word vector for a word which we saw only once in the entire corpus. Here we will take only words which appear at least five times. text2vec provides additional options to filter vocabulary (see <code>?prune_vocabulary</code>).</p>
<pre><code class="language-r">vocab &lt;- prune_vocabulary(vocab, term_count_min = 5L)
</code></pre>
<p>Now we have 71,290 terms in the vocabulary and are ready to construct term-co-occurence matrix (TCM).</p>
<pre><code class="language-r"># Use our filtered vocabulary
vectorizer &lt;- vocab_vectorizer(vocab)
# use window of 5 for context words
tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 5L)
</code></pre>
<p>Now we have a TCM matrix and can factorize it via the GloVe algorithm.<br />
text2vec uses a parallel stochastic gradient descent algorithm. By default it will use all cores on your machine, but you can specify the number of cores if you wish.</p>
<p>Let’s fit our model. (It can take several minutes to fit!)</p>
<pre><code class="language-r">glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
# INFO  [09:35:20.779] epoch 1, loss 0.1758 
# INFO  [09:35:28.212] epoch 2, loss 0.1223 
# INFO  [09:35:35.500] epoch 3, loss 0.1081 
# INFO  [09:35:43.100] epoch 4, loss 0.1003 
# INFO  [09:35:50.848] epoch 5, loss 0.0953 
# INFO  [09:35:58.593] epoch 6, loss 0.0917 
# INFO  [09:36:06.346] epoch 7, loss 0.0890 
# INFO  [09:36:14.123] epoch 8, loss 0.0868 
# INFO  [09:36:21.862] epoch 9, loss 0.0851 
# INFO  [09:36:29.610] epoch 10, loss 0.0836 
</code></pre>
<p>And now we get the word vectors:</p>
<pre><code class="language-r">wv_context = glove$components
word_vectors = wv_main + t(wv_context)
</code></pre>
<p>We can find the closest word vectors for our <em>paris - france + germany</em> example:</p>
<pre><code class="language-r">berlin &lt;- word_vectors[&quot;paris&quot;, , drop = FALSE] - 
  word_vectors[&quot;france&quot;, , drop = FALSE] + 
  word_vectors[&quot;germany&quot;, , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = berlin, method = &quot;cosine&quot;, norm = &quot;l2&quot;)
head(sort(cos_sim[,1], decreasing = TRUE), 5)
#     paris    berlin    munich    madrid   germany 
# 0.7859821 0.7410693 0.6490518 0.6216343 0.6160014
</code></pre>
<p>You can achieve much better results by experimenting with <code>skip_grams_window</code> and the parameters of the <code>GloVe</code> class (including word vectors size and the number of iterations).</p>
</div>
<div class="include-after">
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js" defer></script>
</body>
</html>
